{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process job adverts and load to DB\n",
    "\n",
    "This program processes the scraped job ads saved in ./raw/, searches for org.no. in business register, loads the trained neural-net model (plus helper-objects), estimates ISCO08, and saves it to a pandas data-frame. This then gets inserted into a MySQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading necessary libraries (and probably some unnecessary ones...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "import time\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from elasticsearch import Elasticsearch\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "import html\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input and output folders for files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_path = '/home/radbrt/notebooks/hackathon/job_vacancies/raw/'\n",
    "out_path = '/home/radbrt/notebooks/hackathon/job_vacancies/processed/'\n",
    "error_path = '/home/radbrt/notebooks/hackathon/job_vacancies/errorfiles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to clean up ad-text (remove stopwords, convert to lowercase, remove numbers etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download stopwords if not done yet\n",
    "# nltk.download(\"stopwords\")\n",
    "stops = set(stopwords.words(\"norwegian\"))\n",
    "\n",
    "def alphanum(text):\n",
    "    return re.sub(\"[^A-Za-zæÆøØåÅ]\", \" \", text)\n",
    "\n",
    "def remove_stops(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stops])\n",
    "\n",
    "def clean_text(text):\n",
    "    cleantext = text.lower()\n",
    "    cleantext = alphanum(cleantext)\n",
    "    cleantext = remove_stops(cleantext)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to search for company name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def essearch(name):\n",
    "    try:\n",
    "        res = es.search(index='er', body={\"query\": {\"multi_match\": {\"query\": name, \"fields\": [\"navn\"]}}})\n",
    "        return res['hits']['hits'][0]['_source']['orgnr']\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load neural-net models, tokenizer and labelencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('model.h5')\n",
    "\n",
    "with open('tokenizr.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    \n",
    "with open('labelencoder.pickle', 'rb') as handle:\n",
    "    labelencoder = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that inputs advert-text, returns predicted ISCO08 code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def npred(text):\n",
    "    t_hot = tokenizer.texts_to_matrix([text], mode='binary')\n",
    "    pc = model.predict_classes(t_hot, batch_size=1)\n",
    "    isco = labelencoder.inverse_transform(pc)\n",
    "    return isco[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading files from /raw and processing them, limit number of files to reduce risk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [file for file in os.listdir(in_path) if file.startswith('jobb')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jd = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either connect to docker instance of elasticsearch, or separate, permanent instance with real password, saved outside repo. Object names are the same though, comment out %run or the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ../connect_elastic.py\n",
    "# es = Elasticsearch(['http://localhost:9200'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    try:\n",
    "        e = {}\n",
    "        f = open(in_path + file, 'r').read()\n",
    "        funescape = html.unescape(f)\n",
    "        soup = BeautifulSoup(funescape, 'html.parser')\n",
    "        arbeidsgiver = soup.find_all('h1', attrs={'id': 'arbeidsgivernavn'})[0].text\n",
    "\n",
    "        stillingsinfo = soup.find_all('section', 'stillingsinfo')\n",
    "        stillingsinfo_tittel = stillingsinfo[0].find_all('h1', attrs={'id': 'tittelvalue'})[0].get_text()\n",
    "        stillingsinfo_text = stillingsinfo[0].find_all('p')[0].get_text()\n",
    "\n",
    "        detaljer = soup.find_all('table', 'stillingsdetaljer')\n",
    "        e['title'] = stillingsinfo_tittel\n",
    "        e['text'] = clean_text(stillingsinfo_text)\n",
    "        for element in detaljer[0].find_all('td'):\n",
    "            try:\n",
    "                h1 = element.find_all('h1')[0].get_text()\n",
    "                p = element.find_all('p')[0].get_text()\n",
    "            except:\n",
    "                h1='X'\n",
    "                p = 'Y'\n",
    "            e[h1] = p\n",
    "\n",
    "        e['orgnr'] = essearch(arbeidsgiver)\n",
    "        e['isco08'] = str(npred(e['text']))\n",
    "\n",
    "\n",
    "        jd.append(e)\n",
    "        os.rename(in_path + file, out_path + file)\n",
    "    except:\n",
    "        os.rename(in_path + file, error_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert some column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dateconvert(dstring):\n",
    "    pattern = '\\d{2}\\.\\d{2}\\.\\d{4}'\n",
    "    r = re.search(pattern, dstring)\n",
    "    return datetime.datetime.strptime(r.group(0), '%d.%m.%Y').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'Antall stillinger': 'num_vacancies',\n",
    "                  'Registrert': 'valid_from',\n",
    "                  'Siste publiseringsdato': 'valid_to',\n",
    "                  }, inplace=True)\n",
    "\n",
    "df = df[['num_vacancies', 'valid_from', 'valid_to', 'isco08', 'orgnr', 'text', 'title']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['valid_from'] = df['valid_from'].apply(dateconvert)\n",
    "df['valid_to'] = df['valid_to'].apply(dateconvert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['num_vacancies'] = pd.to_numeric(df['num_vacancies'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either connect to docker instance of mysql, or separate, permanent instance with real password, saved outside repo. Object names are the same though, comment out %run or the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ../connect_mysql.py\n",
    "# engine = create_engine('mysql+mysqlconnector://<user>:<password>@localhost/nav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert to SQL via pandas to_sql method. Failsafe saving to disk with datestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df.to_sql('vacancies', engine, if_exists='append', index=False)\n",
    "except:\n",
    "    df.to_csv('vacancies' + datetime.datetime.now().strftime('%Y%m%d%H%M%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it, folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
